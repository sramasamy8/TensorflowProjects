#INITIALIZE VARIABLES AND PLACEHOLDERS
#beta = 0.003
keep_prob = tf.placeholder("float")
num_features = train_data.shape[1]
num_hidden_units1 = 256
num_hidden_units2 = 256
num_hidden_units3 = 256
num_classes = 7
minibatch_size = 512
global_step = tf.Variable(0, trainable=False)
starter_learning_rate = 0.001
learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,train_data.shape[0]//minibatch_size, 0.97, staircase=True)
x = tf.placeholder("float", [None,num_features])
y = tf.placeholder("float", [None,num_classes])
W1 = tf.Variable(tf.truncated_normal([num_features,num_hidden_units1],stddev=1./math.sqrt(num_features)))
b1 = tf.Variable(tf.zeros([num_hidden_units1]))
W2 = tf.Variable(tf.truncated_normal([num_hidden_units1,num_hidden_units2],stddev=1./math.sqrt(num_hidden_units1)))
b2 = tf.Variable(tf.zeros([num_hidden_units2]))
W3 = tf.Variable(tf.truncated_normal([num_hidden_units2,num_hidden_units3],stddev=1./math.sqrt(num_hidden_units2)))
b3 = tf.Variable(tf.zeros([num_hidden_units3]))
W4 = tf.Variable(tf.truncated_normal([num_hidden_units3,num_classes],stddev=1./math.sqrt(num_hidden_units3)))
b4 = tf.Variable(tf.zeros([num_classes]))

#NEURAL NETWORK EQUATIONS
z1 = tf.nn.relu(tf.matmul(x,W1))
batch_mean1, batch_var1 = tf.nn.moments(z1,[0])
scale1 = tf.Variable(tf.ones([num_hidden_units1]))
beta1 = tf.Variable(tf.zeros([num_hidden_units1]))
z1_n = tf.nn.batch_normalization(z1,batch_mean1,batch_var1,beta1,scale1,1e-3)
a1 = tf.nn.relu(z1_n)
a1_drop = tf.nn.dropout(a1, keep_prob)

z2 = tf.nn.relu(tf.matmul(a1_drop,W2))
batch_mean2, batch_var2 = tf.nn.moments(z2,[0])
scale2 = tf.Variable(tf.ones([num_hidden_units2]))
beta2 = tf.Variable(tf.zeros([num_hidden_units2]))
z2_n = tf.nn.batch_normalization(z2,batch_mean2,batch_var2,beta2,scale2,1e-3)
a2 = tf.nn.relu(z2_n)
a2_drop = tf.nn.dropout(a2, keep_prob)

z3 = tf.nn.relu(tf.matmul(a2_drop,W3))
batch_mean3, batch_var3 = tf.nn.moments(z3,[0])
scale3 = tf.Variable(tf.ones([num_hidden_units3]))
beta3 = tf.Variable(tf.zeros([num_hidden_units3]))
z3_n = tf.nn.batch_normalization(z3,batch_mean3,batch_var3,beta3,scale3,1e-3)
a3 = tf.nn.relu(z3_n)

yhat = tf.matmul(a3,W4) + b4
#cross_entropy_cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = yhat,labels = y))
#BEST WEIGHTS : 
#classes_weights = tf.constant([1.3,0.70,0.92,1.26,1.24,1.7,1.8])
classes_weights = tf.constant([1.3,0.68,0.92,1.31,1.24,2.1,2.3])
cross_entropy_cost = tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(logits=yhat, targets=y, pos_weight=classes_weights))

#regularizer1 = tf.nn.l2_loss(W1)
#regularizer2 = tf.nn.l2_loss(W2)
#cost = tf.reduce_mean(cross_entropy_cost + beta * regularizer1 + beta * regularizer2)
optimization_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy_cost,global_step=global_step)
   
#CREATE SESSION AND INITIALIZE IT
sess = tf.InteractiveSession()
sess.run(tf.global_variables_initializer())

#RUN GRADIENT DESCENT
epochs = 8
print_interval=1
train_acc = np.zeros(epochs//print_interval) 
test_acc = np.zeros(epochs//print_interval)
cost = np.zeros(epochs//print_interval)
k=0
for i in range(epochs):
    print("\nIteration number : " , i)
    print("Learning rate : " , learning_rate.eval(session = sess))
    if(i % print_interval == 0 ):
        cost[k] = sess.run(cross_entropy_cost,feed_dict = {x: train_data,y: processed_train_labels, keep_prob: 1.0})
        train_acc[k] = compute_accuracy(train_data,processed_train_labels)
        test_acc[k] = compute_accuracy(test_data,processed_test_labels)       
        print("Cost : " , cost[k])       
        print("Training accuracy :" ,train_acc[k],"Testing accuracy :" ,test_acc[k]) 
        k+=1
    for j in range(train_data.shape[0]//minibatch_size):#converts to the lowest integer
        train_data_mini = train_data[j*minibatch_size:(j+1)*minibatch_size,:]
        processed_train_labels_mini = processed_train_labels[j*minibatch_size:(j+1)*minibatch_size]
        sess.run(optimization_step,feed_dict={ x: train_data_mini,y: processed_train_labels_mini, keep_prob: 0.9})
